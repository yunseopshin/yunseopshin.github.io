---
lyaout: single
title: '일반화 선형 모델(generalized linear model)의 이해'
categories: coding
tag: [R, 통계 ,blog, jekyll]
author_profile: false
sidebar:
    nav: "docs"
use_math: true
---

# Introduction

주어진 x변수로 y값을 예측하는 경우를 생각해 보자. 이럴경우 y가 연속하는
값을 가지고 정규분포를 따른다면 가장 간단한단 모형으로 선형회귀모형을
생각할 수 있을 것이다. 그런데 그렇지 않고 y가 0, 1 두가지 값을 가지는
경우라면 어떻게 하겠는가? 생각할 수 있는 가장 간단한 방법은
$E(y) = P(y=1) = \beta_{0} + \beta_{1}x$ 이렇게 모형을 만드는 걸 생각할
수 있을것이다.

그런데 이렇게 모형을 만들 경우 생기는 문제점은 일단 $P(y=1)$ 이 값은
0에서 1사이의 값을 가지지만 $\beta_{0} + \beta_{1}x$ 이 값은 실수
전체값을 가질 수 있어서 극단적으로 확률값이 1보다 크거나 0보다 작은
모순적인 상황이 발생할 수 있다. 이러한 문제를 다루기 위해서 우리는
일반화 선형 모델(generalized linear model)을 알아볼 것이다.

# 일반화 선형 모델

## 구성 요소

본격적으로 들어가기에 앞서서 일반화 선형 모델을 구성하는 요소들에 대해서
알아보도록 하자.

1.  확률 요소(random component)

독립적(independent)인 관측값(response variable)
$(y_{1}, y_{2},...,y_{n})$ 들을 의미한다. 이때 이들의 분포는 특정
지수족을 따르고 동일한 분포를 따를 필요는 없다(non identical).

여기서 지수족을 따른 다는 말은 각 $y_{i}$ 가

$$
\begin{equation}
\begin{aligned}
f(y_{i}; \theta_{i}, \phi) = \exp \left(\frac{y_{i}\theta_{i} - b(\theta_{i})}{a(\phi)} +c(y_{i}, \phi) \right)
\end{aligned}
\end{equation}
$$

이런 분포를 따른다는것을 의미한다.

여기서 $\phi$는 dispersion parameter라고 하며 이는 분산과 관련이 있다.

그리고 $\theta_{i}$는 natural parameter라고 하며 여기서 각 $y_{i}$마다
다름을 확인하자. 즉 $(y_{1}, y_{2},...,y_{n})$는 독립지만 동일한 분포를
따르진 않는다는것을 명심하자.

2.  시스템 요소(systematic component) 혹은 선형 예측자(linear predictor)

예측변수(explanatory variable)과 그에 의한 선형함수를 의미한다.
선형회귀모형에서 $X\beta$를 의미한다 보면 된다. 이때 X는 확률변수가
아니라 상수라고 가정한다.

3.  연결 함수(link function)

확률요소와 시스템 요소를 연결해주는 함수이다. 확률요소의 평균의 함수로써
이것을 시스템요소 같다고 둔다.

여기서 $g(u_{i}) = \theta_{i}$를 만족하는 연결 함수를 canonical link
function이라하며 주로 이것을 사용해서 문제를 해결한다.

또한 연결 함수는 미분가능한 단조함수이다.

각 요소들의 예를 생각하면 선형 회귀 모형에서 우리가 일반적으로
$y_{i} = x_{i}'\beta + \epsilon_{i} \quad \epsilon_{i} \sim N(0, \sigma^2)$
이렇게 모형을 만들었을때, 이것은 $E(y_{i}) = u_{i} =x_{i}'\beta$ 로 볼
수 있으므로 $g(u)=u$이고 시스템 요소는 $x_{i}'\beta$이다. 또한,
$y_{i}$의 분포를 보면

$$
\begin{equation}
\begin{aligned}
f(y;u, \sigma) = \exp \left (\frac{uy - \frac{u^{2}}{2}}{\sigma^{2}} \right) - \frac{y^{2}}{2\sigma^{2}} - \frac{1}{2}log(2\pi\sigma^{2})
\end{aligned}
\end{equation}
$$

이다.

여기서,

$$
\begin{equation}
\begin{aligned}
&\theta = u \\
&b(\theta) = b(u) = u^{2} /2 \\
&\phi = \sigma^{2} \Rightarrow a(\sigma^{2}) = \sigma^{2}
\end{aligned}
\end{equation}
$$

이 성립한다.

특히 중요히 봐야할 것은, $\theta\,,b(\theta)$ 이 둘을 유심히 보면 좋을것
같다.

## 확률 요소(random component)의 성질

다음으로 넘어가기 전에 확률 요소 $y_{i}$와 그 지수족 분포의 성질에
대해서 조금만 생각해보고 가도록하자. 여기서 각 지수족 분포의 점수
함수(score function)에 대해서 생각해보면
$L_{i} = \log f(y_{i};\theta,\phi) =\frac{y_{i}\theta_{i} - b(\theta_{i})}{a(\phi)} +c(y_{i}, \phi)$
이므로,

$$
\begin{equation}
\begin{aligned}
& \frac{\partial \log f(y_{i};\theta,\phi)}{\partial \theta_{i}} = \frac{y_{i} - b'(\theta_{i})}{a(\phi)} \\
&\frac{\partial^{2} \log f(y_{i};\theta,\phi)}{\partial \theta_{i}^{2}} =\frac{- b''(\theta_{i})}{a(\phi)} 
\end{aligned}
\end{equation}
$$ 이다. 또한 수리통계 내용에서 점수확률의 평균은 0이고 분산은 정보량이
됨이 알려져 있으므로, 다음이 성립한다

$$
\begin{equation}
\begin{aligned}

& E\left( \frac{\partial \log f(y_{i};\theta,\phi)}{\partial \theta_{i}} \right) = 0 \\


& E\left( \left\{ \frac{\partial \log f(y_{i};\theta,\phi)}{\partial \theta_{i}} \right\}^{2} \right) = E \left(-\frac{\partial^{2} \log f(y_{i};\theta,\phi)}{\partial \theta_{i}^{2}} \right)
\end{aligned}
\end{equation}
$$

이를 통해서 다음이 성립함을 알 수 있다.

$$
\begin{equation}
\begin{aligned}
& u_{i} = b'(\theta_{i}) \\

& Var(Y_{i}) = a(\phi)b''(\theta_{i})

\end{aligned}
\end{equation}
$$

이것이 왜 필요하냐고 의문이 들 수 있는데 그것에 대해서 설명을 하자면
이로부터 해당 모델의 canonical link function을 유도가능하다. 위의 식의
첫번째 내용으로 부터 $b'^{-1}(u_{i}) = \theta_{i}$ 이므로
$g = b'^{-1}$가 이 모델의 cononical link function이 된다.

## 모델 설정

각 i번째 관측치에 대해서 설명변수
$x_{i} = (x_{i1}, x_{i2}, ...,x_{ip})^{t}$ 가 주어졌다고 하자. 그리고
$\beta = (\beta_{1}, \beta_{2},...,\beta_{p})$가 추정해야 하는 모수이다.

이때, 일반화 선형 모델은

$$
\begin{equation}
\begin{aligned}
g(u_{i}) = \eta_{i} = x_{i}'\beta, \qquad i=1,...,n
\end{aligned}
\end{equation}
$$

혹은 행렬 표현법에 의해서

$$
g(u) = \eta = X\beta
$$

이렇게 주어진다. 여기서 $X = (x_{1}, x_{2},..,x_{n})^T$는 $n * p$ 모델
행렬이다.

**이 모형에서 주의해야 할점은 우변에 오차항이 붙지 않는다는 점이다.
좌변이 평균 $u$에 대한 함수로서 고정된 값이므로 우변 또한 고정된 값이
되어야하므로 확률변수인 오차항이 있으면 안된다.**

## 최대가능도 추정(maximum likelyhood estimate)

이제 이렇게 만든 모델에서 우리는 $\beta$를 추정하는게 목적이다. 여기서
각 $y_{i}$들의 분포가 주어졌고 서로 독립이므로 그 결합확률분포를 구할 수
있으므로 최대가능도 추정법을 사용해서 $\beta$를 추정할것이다.

그러기 위해서 가능도함수를 최대화하면 되는데 여기서 지수족의 곱의
형태이므로 로그를 취한 로그가능도함수를 구해서 그것을 최대로 하는
$\beta$를 찾으면 된다. 로그가능도함수는 다음과 같다.

$$
\begin{equation}
\begin{aligned}
L(\beta) = \sum_{i=1}^{n} L_{i} = \sum_{i=1}^{n} \log f(y_{i};\theta,\phi) = \sum_{i=1}^{n} \left\{ \frac{y_{i}\theta_{i} - b(\theta_{i})}{a(\phi)} +c(y_{i}, \phi) \right\} 
\end{aligned}
\end{equation}
$$

여기서 이제 $L(\beta)$의 gradient가 0이되는 $beta$를 찾기 위해서는
각각의 $\beta_{j}$에 관해서, $\frac{\partial L}{\partial \beta_{j}}$를
구해야 하는데 $L(\beta)$의 식을 보면 알 수 있듯이 이식은 $\beta_{j}$에
대해서 바로 들어난 식이 아니다. 즉 저걸 구하기 위해서는 연쇄법칙(chain
rule)을 사용해야한다.

이쯤 우리에게 주어진게 무엇이 있는지 확인하고 가보자.

$$
\begin{equation}
\begin{aligned}
& L(\beta)  = \sum_{i=1}^{n} \left\{ \frac{y_{i}\theta_{i} - b(\theta_{i})}{a(\phi)} +c(y_{i}, \phi) \right\} \\
& u_{i} = b'(\theta_{i}) \\
& g(u_{i}) = \eta_{i} \\
& \eta_{i} = x_{i}^{t}\beta
\end{aligned}
\end{equation}
$$

이 식들이 우리에게 주어진 조건들이다. 여기에 연쇄법칙을 적용하면,

$$
\begin{equation}
\begin{aligned}
\frac{\partial L}{\partial \beta_{j}} = \frac{\partial L}{\partial \theta_{i}} \frac{\partial \theta_{i}}{\partial u_{i}} \frac{\partial u_{i}}{\partial \eta_{i}} \frac{\partial \eta_{i}}{\partial \beta_{j}} \, \qquad j=1,2...,p
\end{aligned}
\end{equation}
$$

이 성립한다. 여기서 index를 주의하도록하자. $i$는 1부터 n까지 가는
표본을 나타내는 index이고 $j$는 1부터 p까지 가는 특성의 개수를 나타내는
index이다. 이 둘을 잘 구분하도록 하자.

위의 것들을 정리하고 마지막으로 $Var(Y_{i}) = a(\phi)b''(\theta_{i})$ 을
사용하면 다음과 같은 정규방정식(normal equation)을 구할 수 있다.

$$
\begin{equation}
\begin{aligned}
\sum_{i=1}^{n} \left\{ \frac{(y_{i} - u_{i})x_{ij}}{Var(Y_{i})} \frac{\partial u_{i}}{\partial \eta_{i}} \right\}=0 \, \qquad j=1,2...,p 
\end{aligned}
\end{equation}
$$

문제점은 이게 닫힌 형태 (closed form)이 존재하지 않아서 $\beta$를
명시적으로 구할 수가 없다는 없다는 점이다.

여기서 이 문제를 해결하는 세가지 방법이 알려져 있다.

1.  뉴턴-랩손 방법(Newton-Raphson Algorithm)

2.  Fisher scoring method.

3.  반복재가중최소제곱법.(Iteratiely reweighted least square, IRLS)

우리가 만약 모델을 만드는데 있어서 일반적인 연결함수가 아닌 canonical
link function을 사용했을 경우에 세가지 방법이 모두 동치가됨이 알려져
있다. 다음절에 이를 증명해보자.

## 반복재가중최소제곱법.(Iteratiely reweighted least square, IRLS)

여기서 canonical link function을 사용했을 경우 뉴턴-랩손 방법이 Fisher
scoring method가 되고 이를 풀었을때 반복재가중최소제곱법이 됨을 간략하게
설명할 것이다.

뉴턴-랩손 방법으로 $L(\beta)$가 최대가 되는 $\beta$를 얻기 위해서는

$$
\begin{equation}
\begin{aligned}

\hat{\beta}^{(m)} = \hat{\beta}^{(m-1)} - \left( \left. \frac{\partial^{2} L}{\partial \beta^{2}} \right|_{\beta=\hat{\beta}_{j-1}} \right)^{-1} \left(\left. \frac{\partial L}{\partial \beta} \right|_{\beta=\hat{\beta}_{j-1}} \right)

\end{aligned}
\end{equation}
$$

을 반복적으로 시행하면 된다.

이 경우 편의상
$U^{(m-1)} = \left(\left. \frac{\partial L}{\partial \beta} \right|_{\beta=\hat{\beta}_{j-1}} \right)$
라고 하자. 또한 여기서 canonical link를 사용했을 경우 $\theta = \eta$가
되므로
$\frac{\partial L_{i}}{\partial \beta_{j}} = \frac{(y_{i} - u_{i})x_{ij}}{a(\phi)}$
이 된다.

이로부터
$\frac{\partial^{2} L_{i}}{\partial \beta_{k} \partial \beta_{j}} = -\frac{x_{ik}x_{ij}}{a(\phi)} \frac{\partial u_{i}}{\partial \eta_{i}}$
이 됨을 알 수있다.

이때, 이 값이 확률변수 $y_{i}$를 포함하지 않으므로
$E \left( \frac{\partial^{2} L_{i}}{\partial \beta_{k} \partial \beta_{j}} \right) = \frac{\partial^{2} L_{i}}{\partial \beta_{k} \partial \beta_{j}}$
이 성립하므로 뉴턴-랩손 방법과 Fisher scoring method는 동치이다.

이제 Fisher scoring method와 반복재가중최소제곱법이 동치임을 유도해보자.

$$
\begin{equation}
\begin{aligned}

\hat{\beta}^{(m)} = \hat{\beta}^{(m-1)} + (I(\hat{\beta^{(m-1)}}))^{-1} \left(\left. \frac{\partial L}{\partial \beta} \right|_{\beta=\hat{\beta}_{j-1}} \right)

\end{aligned}
\end{equation}
$$

를 반복해서 구해야한다.

이때 $I(\hat{\beta^{(m-1)}})$의 $(k, j)$ 번째 항을 보면

$$
E\left(-\frac{\partial^{2} L_{i}}{\partial \beta_{k} \partial \beta_{j}}\right) = \frac{x_{ik}x_{ij}}{a(\phi)} \frac{\partial u_{i}}{\partial \eta_{i}} = \frac{x_{ik}x_{ij}}{Var(Y_{i})} \left(\frac{\partial u_{i}}{\partial \eta_{i}}\right)^2 
$$

이렇게 구해진다.

이 경우에,
$w_{i} = \frac{1}{Var(Y_{i})} \left(\frac{\partial u_{i}}{\partial \eta_{i}}\right)^2$
이고 $W = diag(w_{i})$라고 하면, $I(\beta) = X'WX$ 이렇게 쓸 수 있다.

$$
\frac{\partial L_{i}}{\partial \beta_{j}} = \frac{(y_{i} - u_{i})x_{ij}}{Var(Y_{i})} \frac{\partial u_{i}}{\partial \eta_{i}}
$$

이므로,
$(y_{i} - u_{i}) \frac{\partial u_{i}}{\partial \eta_{i}} = v_{i}, \, v=(v_{1},v_{2},..,v_{n})$라
하면 $\frac{\partial L}{\partial \beta} = X'Wv$가 성립한다. 즉, 주어진
식이

$$
\begin{equation}
\begin{aligned}

& \hat{\beta}^{(m)} = \hat{\beta}^{(m-1)} + (X'WX)^{-1} X'Wv \\

& (X'WX)\hat{\beta}^{(m)} = (X'WX)\hat{\beta}^{(m-1)} +  X'Wv \\

& X'WX\hat{\beta}^{(m)} = X'W(X\hat{\beta}^{(m-1)} + v) \\

& \hat{\beta}^{(m)} = (X'WX)^{-1}X'Wz \qquad (z=X\hat{\beta}^{(m-1)} + v)

\end{aligned}
\end{equation}
$$

이렇게 되므로 Fisher scoring method와 반복재가중최소제곱법이
동치임을보였다.
