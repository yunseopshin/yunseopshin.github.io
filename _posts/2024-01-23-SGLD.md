---
layout: single
title: '논문리뷰: Bayesian Learning via Stochastic Gradient Langevin Dynamics'
categories: machine_learning
tag: [딥러닝, 불확실성, python]
author_profile: false
sidebar:
    nav: "docs"
use_math: true
---

### 목차

- Introduction
- Preliminaries
  - 확률적 경사 하강법 (Stochastic Gradient Descent)
  - 랑게빈 몬테 카를로 (Langevin Monte Carlo)
- Stochastic Gradient Langevin Dynamics
- 예시 코드
- 결론
- 참고자료


## Introduction

Bayesian learning을 하는데 있어서 가장 문제가 되는 것은 사후분포(posterior distribution)가 closed form이나 잘 알려진 형태로 구해지지 않아서 다루기 어렵다는 것이다. 이를 다루기 위한 한가지 방법으로 사후분포에서 표본(sample)을 추출해 이것의 표본 통계량으로 사후분포의 통계량을 근사하는 것이다 (Strong Law of Large Number). 하지만 여기서 사후분포의 차원이 일차원이 아닌 다차원인 경우에 사후분포로부터 독립인 표본을 추출하는 것이 어려워, markov chain인 표본을 추출해 이를 통해 사후분포를 근사한다 (Ergodicity). 

가장 잘 알려진 MCMC 방법론으로는 깁스 추출법(Gibbs sampling)과 해밀턴 몬테 카를로(Hamiltonian Monte Carlo)가 있다. 다만 이러한 방법들의 경우 데이터 셋이 커질시 계산시간이 오래걸리는 것이 알려져있다.

여기서는 큰 데이터 셋에 적합하기 위한 scalable한 방법론을 제시하기 위해 해밀턴 몬테 카를로의 변형인 랑게빈 몬테 카를로 (Langevin Monte Carlo)와 확률적 경사 하강법(Stochastic Gradient Descent)을 결합한다. 


## Preliminaries

이번 챕터에서는 본격적인 논의에 들어가기 앞서 사전지식들에 대해 간단하게 알아보도록 하자.

기본적인 가설로 $\theta$ 를 사전분포 $p(\theta)$ 를 따르는 모수라 하고, $p(x  \mid \theta)$ 를 $\theta$ 가 주어졌을 때 $x$ 의 조건부 분포라 하자. 그리고 $N$ 개의 데이터 $X = [ x_{i} ]_{i=1}^{N}$ 이 관측되었다고 하자. 이 때 사후분포는

$$
\begin{align}
p(\theta | X) \propto p(\theta) \prod_{i=1}^{N} p(x_{i}| \theta)
\end{align}
$$

로 구해진다. 또한 곱 보다는 합을 다루는게 쉬우므로 여기에 로그를 취한

$$
\begin{align}
\log p(\theta | X) \propto \log p(\theta)  +  \sum_{i=1}^{N}  \log p(x_{i}| \theta)
\end{align}
$$

를 많이 사용한다.

보통의 머신러닝이나 답러닝의 경우 (2) 을 최대로 하는 "Maximum a posterior" (MAP)를 구해 $\theta$를 추정하고, bayesian learning에서는 저 분포로 부터 표본을 추출해 사후분포를 추정한다.


### 확률적 경사 하강법 (Stochastic Gradient Descent)

경사 하강법(Gradient Descent)은 MAP나 MLE(Maximum Likelyhood Estimator) 즉 특정식에서 최댓값(최솟값)을 구하는 알고리즘이다. 이를 큰 데이터셋에 더 효율적으로 적용하기 위해서 각 반복시에 데이터 셋을 랜덤하게 mini-batch 단위로 나눠 그것만으로 각 시행에서 update 하는것을 확률적 경사 하강법(SGD)라고 한다. 이는 보통의 경우 최적화에서 가장 많이 사용되는 알고리즘이다. 각 반복 $t$ 마다 업데이트 되는 $\theta$값은 다음과 같다.

$$
\begin{align}
\Delta \theta_t=\frac{\epsilon_t}{2}\left(\nabla \log p\left(\theta_t\right)+\frac{N}{n} \sum_{i=1}^n \nabla \log p\left(x_{t i} \mid \theta_t\right)\right)
\end{align}
$$

이 때, local maximum으로 잘 수렴하기 위해서는 step size $\epsilon_t$가 다음 조건을 만족해야 함이 알려져 있다.

$$
\begin{align}
\sum_{t=1}^{\infty} \epsilon_t=\infty \quad \sum_{t=1}^{\infty} \epsilon_t^2<\infty
\end{align}
$$

다만 이렇게 구한 MAP, MLE 추정량은 불확실성(uncertainty)를 추정하기 어렵고, overfitting의 가능성이 있다는 단점이 있다.

### 랑게빈 몬테 카를로 (Langevin Monte Carlo)

(1)로부터 샘플을 추출하는 방법 중 한개로 Langevin 동역학으로 부터 아이디어를 얻은 랑게빈 몬테 카를로가 있다. 이는 해밀턴 몬테 카를로를 변형한 방법으로 $\theta$를 다음과 같이 업데이트 해 마코프 체인을 얻는다.

$$
\begin{align}
\Delta \theta_t & =\frac{\epsilon}{2}\left(\nabla \log p\left(\theta_t\right)+\sum_{i=1}^N \nabla \log p\left(x_i \mid \theta_t\right)\right)+\eta_t  \nonumber \\
\eta_t & \sim N(0, \epsilon I)
\end{align}
$$

이로부터 $\theta * = \theta_{t} + \Delta \theta_t$ 로 한뒤 메트로폴리스-헤이스팅스(Metropolis-Hastings) 알고리즘을 적용해 합격하면 $\theta_{t+1}  = \theta *$ 그렇지 않으면 $\theta_{t+1}  = \theta_{t}$로 업데이트한다.

## 6. 참고자료

    1. Van der Maaten, Laurens, and Geoffrey Hinton. "Visualizing data using t-SNE." Journal of machine learning research 9.11 (2008).
    2. Hinton, Geoffrey E., and Sam Roweis. "Stochastic neighbor embedding." Advances in neural information processing systems 15 (2002).
    3. Wattenberg, et al., "How to Use t-SNE Effectively", Distill, 2016. http://doi.org/10.23915/distill.00002
    4. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html

