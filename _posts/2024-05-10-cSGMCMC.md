---
layout: single
title: 'CYCLICAL STOCHASTIC GRADIENT MCMC FOR BAYESIAN DEEP LEARNING'
categories: deep_learning
tag: [딥러닝, 불확실성, python]
author_profile: false
sidebar:
    nav: "docs"
use_math: true
---


### 목차

- Introduction
- Preliminaries:SG-MCMC
- Cyclical SG-MCMC
- Theoretical Analysis
- Experiment
- Summary

석사 졸업 논문 초고 작성하다 흥미로운 내용이 있어 해당 논문을 공부해보려 한다. 

## 1. Introduction

심층신경망의 가중치의 사후분포는 대부분의 경우 고차원이며 multi-modal(convex하지 않고 여러개의 봉우리가 존재)인 경우가 많다. 그래서 이 사후분포로부터 가중치를 추출하려 하면 국소적인 최대 혹은 최소지점에 갇혀 다른 부분을 표본추출 하지 못하는 경우가 생긴다. 이를 해결하기 위해선 학습률을 크게 잡아야하지만, SGLD를 포함한 확률적 몬테카를로 방법에서는 반복을 진행할 때마다 학습률을 작게해야 수렴성이 보장되기 때문에 지속적으로 학습률을 크게 하기 어렵다는 문제점이 있다.

해당 논문은 이를 해결하기 위해 Cyclical learning rate scheduler를 사용하는 방법을 제안한다.

## 2. Preliminaries:SG-MCMC

SG-MCMC에는 크게 두가지 방법이 있는데 하나는 해밀토니안 몬테카를로를 바탕으로한 SG-HMC와 다른 하나는 랑쥬빈 몬테카를로를 바탕으로한 SG-SGLD가 있다. SG-SGLD에 대해서는 이전에 다룬 포스팅이 있으니 해당 내용 참고해주면 될 것 같다. 핵심만 요약해서 말하면 사후분포로터 표본을 추출하는데 이 경우 전체 데이터에 대한 가능도를 사용하면 너무 계산량이 많아지므로 sgd의 방법론을 받아드려, mini-batch로 데이터를 나눠 이로부터 표본을 추출하는 방법론을 칭한다.

이 때 추출된 표본이 사후분포로 수렴하기 위해서는 다음 가정을 만족해야 한다.

**Assumption 1.** The step size $\{\alpha_{k} \}$ are decreasing, i.e., $0<\alpha_{k+1}< \alpha_{k}$, with $1) \sum_{k=1}^{\infty} \alpha_{k} = \infty ; \text{and} 2)  \sum_{k=1}^{\infty} \alpha_{k}^{2} < \infty$.

## 3. Cyclical SG-MCMC

위 처럼 학습률이 작아지는데 local mode에 갇힌 경우 여기서 벗어나기 위해서는 많은 수의 반복이 필요하다. 즉 다시말해 계산량이 많아지는 문제가 생긴다. 이를 해결하기 위해 해당 논문의 저자는 Cyclical learning rate scheduler를 다음과 같이 코사인 함수를 이용해 제시한다. 

$k$번째 반복의 학습률을 다음과 같이 정의한다.

$$
\begin{align}
\alpha_{k} = \frac{\alpha_{0}}{2} \left[ \cos \left( \frac{\pi \text{ mod}(k-1, K/M)}{K/M} \right) + 1\right],
\end{align}
$$

여기서 $M$ 은 한 주기안에 속하는 반복의 횟수이고, $K$는 전체 반복 횟수를 의미한다. 

흥미로웠던점은 여기서 각 주기마다 burn-in을 적용한다는 점이었다. 각 주기안에서 학습률이 큰 초기에는 사후분포에서 확률밀도가 높은 곳을 찾아가는 단계(Exploration Stage), 그리고 어느정도 반복이 진행되 새로운 local mode를 찾아내고 학습률이 충분히 작아진 다음에는 그 mode를 구체화해 표본을 추출하는 단계(Sampling Stage) 이 두 단계로 나눠서 반복해 표본추출을 진행하는게 가장 흥미로웠다.

이를 Epoch($k$)에 대한 그래프로 나타내면 다음과 같다.

![png](/images/cSGMCMC-files/csgmcmc1.png)


전체 과정을 알고리즘으로 쓰면 다음과 같다.


**Alogorithm 1.** Cyclical SG-MCMC

**Input:** The initial stepsize $\alpha_{0}$, number of cycles $M$, number of training iterations $K$ and proportion of exploration stage $\beta$.

$\quad$ **for** k=1:K do

$\qquad \alpha \leftarrow \alpha_{k}$ according to Eq (1).

$\qquad \mathbf{if} \, \, \frac{ \text{ mod}(k-1, K/M)}{K/M} < \beta \text{ }\mathbf{then}$

$\qquad \quad \% \text{ Exlploration stage}$

$\qquad \quad \theta \leftarrow \theta -\alpha \nabla \tilde{U}_{k} (\theta)$

$\qquad \mathbf{else}$

$\qquad \quad \% \text{ Sampling stage}$

$\qquad \quad \text{ Collect samples using SG-MCMC methods}$

**Output:** Samples $\{\theta_{k} \}$

이전에는 multi-modal분포로 부터 표본추출을 하기 위해서는 초기값을 여러개로 설정해서 여러개의 마코프체인을 추출하는 방법을 사용했었다. 이런 경우 마코프 체인을 $m$개를 추출한다고 하면 체인의 개수 * 체인의 길이 만큼의 계산량이 필요했지만, 위 방법을 사용하면 하나의 체인만으로   multi-modal분포로 부터 표본추출이 가능하기 때문에 계산상에서 이점이 있다.

## 4. Theoretical Analysis

 Cyclical learning rate scheduler를 사용한 SG-MCMC로부터 추출한 표본이 사후분포로 수렴하는것을 두가지 측면에서 보였다. 한 가지는 추출한 표본으로부터의 경험적 분포가 사후분포로 weak convergence 하는 것이고 다른것은 경험적 분포와 사후분포 사이의 거리가 유계가 되는것을 보였다. 여기서 분포사이의 거리는 Wasserstein distance를 사용하였다. 

 해당 파트는 내가 아직 잘 공부하지 못한 측면도 있고 생각을 해봤을때 개인적으로는 납득되지 않는 부분이 있어 해당부분에 대해서 설명해보려고 한다.

### 4.1. Weak Convergence

여기서 저자들이 보인것과 내가 알고있는것과의 괴리감이 있어서 지금 당장은 납득이 잘 가지 않는 상황이다. 유계이고 연속인 함수 $\phi$에 대해서

$$
\bar{\phi} \triangleq \int_{\mathcal{X}} \phi(\theta) \rho(\theta) \mathrm{d} \theta , \quad \hat{\phi}=\frac{1}{K} \sum_{k=1}^K \phi\left(\theta_k\right)
$$

이렇게 정의할 때 다음 정리가 성립한다는걸 저자들이 증명하였다.

**Theorem 1.** Under Assumptions 2 in the appendix, for a smooth test function $\phi$, the bias and MSE of cSGLD are bounded as:
$$
\begin{align}
\text { BIAS: }|\mathbb{E} \tilde{\phi}-\bar{\phi}|=O\left(\frac{1}{\alpha_0 K}+\alpha_0\right), \quad M S E: \mathbb{E}(\tilde{\phi}-\bar{\phi})^2=O\left(\frac{1}{\alpha_0 K}+\alpha_0^2\right)
\end{align}
$$

여기서 $K \rightarrow \infty, \alpha_{0} \rightarrow 0$ 를 해주면 각각이 0으로 수렴하고 이로부터 $\tilde{\phi} \overset{p}{\rightarrow} \bar{\phi}$ 임은 성립하지만 이게 $\tilde{\phi} \rightarrow \bar{\phi} \,\, a.s.$ 임을 의미하지는 않는다. 여기서 weak convergence를 보이기 위해선 저게 확률수렴하는 것이 아닌 그보다 강한 almost surely 수렴해야하는데 여기서 내가 알고있는 것과는 달라 이해가 어렵다. 이는 추후 Appendix를 공부해야 더 잘 이해할 수 있을 것 같다.

### 4.2. Convergence under the Wasserstein distance

두 분포사이의 거리를 Wasserstein distance로 다음과 같이 정의하자.

$$
W_2^2(\mu, \nu):=\inf _\gamma\left\{\int_{\Omega \times \Omega}\left\|\theta-\theta^{\prime}\right\|_2^2 \mathrm{~d} \gamma\left(\theta, \theta^{\prime}\right): \gamma \in \Gamma(\mu, \nu)\right\}
$$

이 때 $\mu_{K}$를 추출된 표본으로부터 만든 경험적 분포의 확률측도로 정의하고, $\nu_{\infty}$를 타겟 사후분포의 확률측도로 정의하면 다음이 성립하는 것을 증명했다.

**Theorem 2.** Under Assumption 3 in the appendix, there exist constants $\left(C_0, C_1, C_2, C_3\right)$ independent of the stepsizes such that the convergence rate of our proposed cSGLD with cyclical stepsize sequence equation 1 is bounded for all $K$ satisfying $(K \bmod M=0)$, as $W_2\left(\mu_K, \nu_{\infty}\right) \leq$
$$
C_3 \exp \left(-\frac{K \alpha_0}{2 C_4}\right)+\left(6+\frac{C_2 K \alpha_0}{2}\right)^{\frac{1}{2}}\left[\left(C_1 \frac{3 \alpha_0^2 K}{8}+\sigma C_0 \frac{K \alpha_0}{2}\right)^{\frac{1}{2}}+\left(C_1 \frac{3 \alpha_0^2 K}{16}+\sigma C_0 \frac{K \alpha_0}{4}\right)^{\frac{1}{4}}\right]
$$

Particularly, if we further assume $\alpha_0=O\left(K^{-\beta}\right)$ for $\forall \beta>1, W_2\left(\mu_K, \nu_{\infty}\right) \leq C_3+$ $\left(6+\frac{C_2}{K^{\beta-1}}\right)^{\frac{1}{2}}\left[\left(\frac{2 C_1}{K^{2 \beta-1}}+\frac{2 C_0}{K^{\beta-1}}\right)^{\frac{1}{2}}+\left(\frac{C_1}{K^{2 \beta-1}}+\frac{C_0}{K^{\beta-1}}\right)^{\frac{1}{4}}\right]$.

여기서 그래서 $K \rightarrow \infty$로 보내면 결국 $W_2\left(\mu_K, \nu_{\infty}\right)$ 가 상수 $C_{3}$로 유계가 되는데 사실 이거도 잘 이해가 안되는데 저게 수렴하는걸 말하려면 0으로 수렴해야지 특정 상수로 유계가 되는걸 보여서 뭐 어쩌겠다는건지도 잘 이해가 되진 않는다. 물론 유계인거만해도 어느정도 잘 근사가 된다고 말할 순 있겠지만, 그게 weak convergence를 나타내냐? 라고 묻는다면 나는 잘 이해가 되지 않는다.



## 5. Inference

4장 까지의 내용을 통해 weight의 사후분포를 근사하는 Variational distribution을 Dropout DNN을 통해 구할 수 있단 걸 구했다. 그렇다면 이것을 어떻게 해야 Bayesian으로 해석 가능할까. 일반적인 Dropout DNN과의 차이는 과연 무엇일까? 그것은 바로 Inference 과정에서 발생한다. Training 시에는 똑같이 진행되지만 Dropout DNN에서는 inference 할 때 Dropout을 적용시키지 않고 한개의 고정된 출력값을 구하는 반면, MC-Dropout에서는 inference시에도 Dropout을 적용한다. 그에 따라 한개의 입력값에 대해 여러 출력값이 나올 수 있다.

쉽게 설명해 MC-Dropout은 한개의 입력값 $x$에 대해 inference를 진행할 때 Dropout을 적용한 여러번의 반복을 통해 여러 출력값 $y_{t}$를 얻고 이것의 평균으로 예측값을 얻어내고, 분산으로 모델 불확실성을 추정한다. 직관적으로 다음 그림을 참고하면 좋을 것 같다.

![png](/images/MCDropout_files/MC_dropout1.png)

다시 말해 $y \mid x$의 분포에서 표본추출하는 과정을 반복하는 것이다. 그리고 그 표본들을 이용해 분포의 통계량들을 추정할 수 있다. 저 표본들의 표본평균과 표본 분산이 실제 평균, 분산의 불편추정량으로 근사할 수 있다는걸 이론적으로 증명할 수 있으나 해당 포스팅의 범위를 넘는다 판단해 여기에 담지 않도록 하겠다. 관심있는 독자들은 (Gal, 2016)의 Appendix를 참고하자.

## 6. Summary

지금까지 MC-Dropout에 대해 정리해봤다. 사실 해당 논문에는 이 내용 의외에도 BNN이 Deep-GP로 수렴한다는 내용을 다뤄 보다 어렵지만, 내 생각에 그건 당장에 필요한 부분은 아닌 것 같아 굳이 다루지 아니 하였다. 의미있는 내용이지만 굳이 같은 논문에서 다룰만큼 잘 어우러지냐? 라고 하면 사실 나는 잘 모르겠더라. 처음 Bayesian neural network를 공부하려 했을때 "처음 공부하는 것이니 인용수가 제일 높은거 부터 읽어보자" 라고 생각해 읽게된 논문이다. 이때만 해도 논문 읽는게 익숙하던 때가 아니었어서 처음 읽을때 엄청 어렵고 해맸던 기억이 나는데 그게 벌써 1년하고도 6개월 전이다. 막상 나중에 선배한테 물어보니 다른 방법론들에 비해 성능이 안좋기 때문에 그렇게 많이 안 쓴다고 한다. 근데 왜 인용수가 높냐고 물어보니 성능이 안좋아 baseline으로 쓰기 좋기때문에 이곳 저곳에 많이 불려나가서 그렇다는데.... 그럼에도 한 번쯤은 정리 해둘 필요가 있을 것 같아 대략적인 내용은 정리 한 것 같다.

## 7. Reference

    1. Gal, Yarin, and Zoubin Ghahramani. "Dropout as a bayesian approximation: Representing model uncertainty in deep learning." international conference on machine learning. PMLR, 2016.