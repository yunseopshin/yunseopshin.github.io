**Prediction with linear model and xgboost, and its Interpretability**
================
Shin Yun Seop



# Abstract

These day, person has many observation and covariates affecting the
result are increasing. So traditional analysis method like linear model
is insufficient to fit the large data set. And developing of computing
power supports the complicated model like boost or neural network. In
this paper we focus on the XGBoost an improved version of gradient
boost.

**keyword**

lasso regression, gradient boost, XGBoost, Large-scale Machine Learning

# Introduction

Machine learning and data-considering approaches are becoming very
important in many areas. Spam classifier of our email by learning from
many amount of junk mail, face recognition learning from many different
picture. And recently, deep learning models are advanced enough to draw
pictures. All these situations are we have many observations and there
are large scale of variables.

Among these machine learning methods, gradient boost is one methods that
is applicable many cases. It is used in regression and classification
also.

In this paper we mainly focus on the XGBoost an improved version of
gradient boost. It is penalized version of gradient boost. It perform
well on Kaggle competition by winning several championships.

The remainder of the paper is organized as follows. We will first think
of linear model, lasso regression with example in Section 3. We then
shortly review the gradient boost in Section 4. And then examine what is
the XGBoost, thinking of its composition in Section 5. The relevant
interpretations are included in Section 6. Next, we apply XGBoost on
real data and interpret the results in Section 7. Finally we conclude
the paper in Section 8. And we use Jeju City traffic data from
2021/09/01 to 2022/07/31 in example section. It contains 4701217
observations and 21 variables. Among these, “start node name, end node
name” is so noisy. And this variable is almost equal to “start latitude,
start longitude, end latitude, end longitude”. So I except them. I
choose 2022/01/01\~ 2022/05/31 data. And randomly divide them as
training and validation data containing each 1790071, 300000
observations. Also, select 2022/06/01 \~ 2022/07/31 data as test set.
This set contain 764832 data. We predict test set’s target(mean of
velocity) using training and validation set.

# Lasso regression

In ordinary linear regression, when we have many variables the model is
so overfitted. So we need to select the variables or shrinkage the
coefficients. Lasso give L1 penalty to lasso function so we do variable
selection and shrinkage at the same time.

## Objective function for lasso regression

The lasso estimate is defined by

and this is equivalent to

by largrangian multiplier method. We simply use mean square loss
function added L1 penalty. In this case, $t$ and $\lambda$ are inverse
proportional relationship. And as $\lambda$ is large, the coefficients
become small. Normally, we select $\lambda$ using cross validation.

## Property of lasso regression

When we think about the (1) as image Figure1 is established. The L1 norm
is represented by diamond so we can make coefficient 0.

<figure>
<img src="lasso1.png" style="width:80.0%" alt="ESL p71" />
<figcaption aria-hidden="true">ESL p71</figcaption>
</figure>

And then each columns of input matrix X are orthogonal following holds.
From figure2 we know that lasso translates each coefficient by a
constant factor $\lambda$, truncating at zero. From these to figure we
know that lasso can variable selection. This is case of orthogonal
variable but when general case, this operates similarly.

<figure>
<img src="lasso2.png" style="width:80.0%" alt="ESL p71" />
<figcaption aria-hidden="true">ESL p71</figcaption>
</figure>

Lasso regression also gives the solution path of the coefficients. From
this we know that coefficients goes to zero as $\lambda$ increases.

<figure>
<img src="lasso3.png" style="width:80.0%" alt="solution path" />
<figcaption aria-hidden="true">solution path</figcaption>
</figure>

## Example

We model ordinary least square and lasso regression to our training set
and get mean square error using test set. In this case I use 10 folds
cross validation to get optimal $\lambda$.

``` r
cv_glmfit <- cv.glmnet(X_train, y_train, alpha = 1)
best_lam_lasso <- cv_glmfit$lambda.min
glm_fit <- glmnet(X_train, y_train, 
                  lambda = best_lam_lasso, alpha = 1)
lasso_pred <- predict(glm_fit, s =best_lam_lasso, 
                      newx = X_test)
lasso_mse <- mean((lasso_pred - y_test)^2)
```

``` r
lm_mse
lasso_mse
```

In this case, test mse of ols is smaller than lasso regression. I think
that this is because we have may observations so we need more flexible
model not penalty model.

# Gradient Boost

Let loss in using $f(x)$ to predict $y$ on the training data is

Our goal is minimize $L(f)$ with respect to $f$, where $f$ is additive
tree model.

## Steepest Desecent

$-g_{m}$ is the local direction in $\mathbb{R}^{N}$.

## Algorithm of gradient boost.

The gradient (4) is trivial to calculate for any differentiable loss
function $L(y,f(x))$. But, unfortunately the gradient (4) is defined
only at the training data points $x_{i}$, whereas the ultimate goal is
to generalize $f_{M}(x)$ to new data not represented in the training
set. A possible solution to this dilemma is approximation using tree to
the negative gradient.

Using this and forward stagewise additive algorithm we derive following
algorithm for gradient boosting. Its specific algorithm is depicted in
Figure 4.

<figure>
<img src="gdb.png" style="width:80.0%" alt="ESL p361" />
<figcaption aria-hidden="true">ESL p361</figcaption>
</figure>

# eXtreme Gradient boost

XGBoost is improved model of gradient boosting to prevent overfitting.
XGBoost penalty to tree size and out put of node’s value to prevent
overfitting. It has several advantage over the existing boosting method.

- XGBoost is faster than gbm
- Learning with parallel method
- Regularization
- Early stopping
- Applying to both regression and classification

And in boosting process, categorical variables are not applicable
directly. So we need to dummy codding of them. This is described in
detail in Figure 5.

<figure>
<img src="machine-learning-dummy-variable-i2tutorials.jpg"
style="width:80.0%"
alt="https://www.i2tutorials.com/what-do-you-mean-by-dummy-variable-where-it-is-used-in-machine-learning/" />
<figcaption aria-hidden="true"><a
href="https://www.i2tutorials.com/what-do-you-mean-by-dummy-variable-where-it-is-used-in-machine-learning/"
class="uri">https://www.i2tutorials.com/what-do-you-mean-by-dummy-variable-where-it-is-used-in-machine-learning/</a></figcaption>
</figure>

## Objective function for XGBoost

Let $L$ is second derivative loss function. Then we optimize,

In this case $T$ is size of tree, and
$||\phi||^{2} = \sum_{j=1}^{T} w_{j}^{2}$, where $w_{j}$ is output of
j-th node. And then let $g_{i}$ is same with gradient boost setting and
let define $h_{i}$ is following.

Then, the model in (6) is equal to following

So, the tree model in m-th reiteration is $T(x, \tilde{\theta}_{m})$.

## Proof of objective function of XGBoost

Second-order talyor approximation can be used to prove it.

$- \frac{1}{2}h_{im}g_{im}^{2} + L(y_{i}, f_{m-1}(x_{i}))$ is constant
to $\theta$ so,

## Loss function in XGBoost

XGBoost can have various loss function with second derivative. Largely,
it is divided into regression and classification case. In regression
setting, we normally use squared loss. But it is vulnerable to outlier,
namely non-robust. XGBoost support pseudohubererror which is transform
of hubererror with with second derivative. Huber error is complementary
for mean and median. It is more robust than square error. So we need to
select the loss function depending on data set.

Various loss function used in XGBoost are confirmed in xgb
document(<https://xgboost.readthedocs.io/en/stable/parameter.html>).

<figure>
<img src="error.png" style="width:70.0%" alt="ESL p351" />
<figcaption aria-hidden="true">ESL p351</figcaption>
</figure>

## Considerable parameter

- Size of trees for boosting

J is the number of terminal node. J is correlated with interaction
effect of each variable. When J=2 the model only have main effect of
$X_{j}$. Similarly when J=3, two-variable interaction effects
$(X_{j}, X_{k})$are also allowed , and so on. Experience so far
indicates that $4 \le J \le 8$ works well in the context of boosting,
However there is no significant improvement over using J=6. Figure 7
show that most simple case overperforms the others. This is
**max.depth** in R parameter.

<figure>
<img src="treesize.png" style="width:75.0%" alt="ESL p363" />
<figcaption aria-hidden="true">ESL p363</figcaption>
</figure>

- The number boosting iterations.

When the number of trees is so large, there is risk of overfitting. We
obtain optimal of this by using validation set or cross validation. This
is **nrounds** in R parameter.

- Learning rate

Learning rate regulates additive effect of each tree. This affects the
process of 2-(d) in Figure 4. This rate is multiplied by front of each
tree. Normally, we set it very small(\< 0.1) and the choose the number
boosting iterations by early stopping. With smaller learning rate we
prevent overfitting but it require more computational power. This is
**eta** in R parameter.

- Subsampling

Its idea come from bagging. Each iteration, we sample **without
replacement** from observation and make tree with them. Normally, we use
half of observations but when observations are so large, then it can be
much smaller. It reduces computation hour and many case fitting more
accurate model. This is **subsample** in R parameter.

- Varialbe sampling

Its idea come from random forest. Each iteration, we sample select only
subset of features. It can reduce covariance of each trees so stabilize
the model. And it reduces computation hour. This is **colsample_bytree**
in R parameter.

First we choose objective(loss function) by problem type. Typically some
early explorations determine suitable values for except the number of
trees, and then obtain optimal value of that using validation set or
cross validation.

# Interpretability of XGBoost

XGBoost is complicate machine learning model. But we use two way for the
interpretation of fitted XGBoost model.

## Relative importance

First is Relative importance of predictor variables. Using this we show
that few of predictor variables influence more on the response than
other variables. This is easily obtained by xgboost package’s function
xgb.importance, xgb.plot.importance.

## Partial dependence plots

Second is Partial dependence plots. The effect of selecting variables on
$f(X)$ after accounting for the effect of the other variables. It is
limited to low-dimensional because we can easily display functions of
one or two arguments and when computing more than one variable it take
hour so large. It is done **pdp** package in R, and it support parallel
operation itself. Note that To do this we must designate *objective =
““* when we fitting the model, and *train* parameter take *data.frame*
not matrix.

# Application to real data set

To do xgboost in R, we weed to make xgb.DMatrix first.

``` r
library(xgboost)
xgb_train <- xgb.DMatrix(data = X_train,
                         label = y_train)
xgb_validation <- xgb.DMatrix(data = X_validation,
                              label = y_validation)
watchlist = list(train = xgb_train, 
                 validation = xgb_validation)
```

Then we fit two case. fit_2 is more constraints version.

``` r
xgb_fit <-
    xgb.train(data = xgb_train, max.depth = 4, 
              nrounds = 5000, watchlist = watchlist,
              eta = 0.5, subsample = 0.4, 
              objective = "reg:squarederror")

xgb_fit_2 <- 
    xgb.train(data = xgb_train, max.depth = 4,
              nrounds = 20000, watchlist = watchlist,
              subsample = 0.4, eta= 0.1, 
              objective = "reg:squarederror",
              colsample_bytree = 1/3)
```

Note that we use square loss in this situation.

``` r
plot(pull(xgb_fit$evaluation_log[,3]), xlab = "number of trees", ylab = "rmse")
points(pull(xgb_fit_0.1_eta_with_colsample$evaluation_log[,3]), col = "red")
```

![](final-project_files/figure-gfm/unnamed-chunk-5-1.jpeg)<!-- -->

From this result first version is better than second. I think this is
because we don’t have sufficient tree for second model. But from the
result first model is reasonably good so I use it. Use it we can get
fiited value for test set and using them we get test mse.

``` r
xgb_test <- xgb.DMatrix(data = X_test, label = y_test)
yhat_xgb <- predict(xgb_fit, xgb_test)
```

``` r
mean((yhat_xgb - y_test)^2)
```

    ## [1] 25.82715

Also we can get Relative importance, and Partial dependence plots.

``` r
importance_matrix = xgb.importance(colnames(xgb_train),
                                   model = xgb_fit)

xgb.plot.importance(importance_matrix,top_n = 10)

xgb.plot.importance(importance_matrix,top_n = 20)
```

<img src="importance1.png" style="width:100.0%" />

<img src="importance2.png" style="width:100.0%" />

``` r
#install.packages("pdp")
libarry(pdp)
p1 <- 
    partial(xgb_fit, pred.var = "maximum_speed_limit",
            plot =TRUE, train = data.frame(X_train))


library(doParallel) # load the parallel backend
cl <- makeCluster(4) # use 4 workers
registerDoParallel(cl) # register the parallel backend

p2 <-
    partial(xgb_fit, pred.var = "end_longitude", 
            plot =TRUE, train = data.frame(X_train),
                 parallel = TRUE)

p3 <- 
    partial(xgb_fit, 
            pred.var = c("maximum_speed_limit", "end_longitude"),
            plot = TRUE, chull = TRUE,
            train = data.frame(X_train), parallel = TRUE)

p4 <- 
    partial(xgb_fit,
            pred.var = c("end_longitude", "end_latitude"), 
            train = data.frame(X_train), 
            parallel = TRUE, chull=TRUE)

p5 <- 
    partial(xgb_fit,
            pred.var = c("start_longitude", "start_latitude"),
            train = data.frame(X_train), 
            parallel = TRUE, chull=TRUE)

stopImplicitCluster() # stop parallel
```

``` r
p1
```

![](final-project_files/figure-gfm/unnamed-chunk-10-1.jpeg)<!-- -->

``` r
p2
```

![](final-project_files/figure-gfm/unnamed-chunk-10-2.jpeg)<!-- -->

``` r
p3
```

![](final-project_files/figure-gfm/unnamed-chunk-10-3.jpeg)<!-- -->

We also use ggmap to describe the real data.

``` r
library(ggmap)
ggmap(mapdata)+
    geom_jitter(data =p4, aes(x=end_longitude, y=end_latitude, color=yhat),
             alpha=0.5 ,size =10) +  scale_color_viridis() +
    theme(axis.text  = element_blank(),
          axis.title   = element_text(size = 15, face="bold"),
          plot.margin  = margin(0.2, 0.2, 0.2, 0.2, "cm"),
          axis.ticks.x = element_blank(),
          axis.ticks.y = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank()) +
    geom_point(data=locationInfo, aes(x =lon, y=lat), col ="red")+
    scale_color_viridis(name = "velocity")+
    geom_text(data=locationInfo, aes(label=Name), size=7, hjust=1.2,
              fontface = "bold", family= "AppleMyungjo") 
    

ggmap(mapdata) +
    geom_jitter(data =p5, aes(x=start_longitude, y=start_latitude, color=yhat),
                alpha=0.5 ,size =10) +  scale_color_viridis() +
    theme(axis.text  = element_blank(),
          axis.title   = element_text(size = 15, face="bold"),
          plot.margin  = margin(0.2, 0.2, 0.2, 0.2, "cm"),
          axis.ticks.x = element_blank(),
          axis.ticks.y = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank()) +
    geom_point(data=locationInfo, aes(x =lon, y=lat), col ="red")+
    scale_color_viridis(name = "velocity")+
    geom_text(data=locationInfo, aes(label=Name), size=7, hjust=1.2,
              fontface = "bold", family= "AppleMyungjo") 
```

<figure>
<img src="end3.png" style="width:100.0%" alt="end_point" />
<figcaption aria-hidden="true">end_point</figcaption>
</figure>

<figure>
<img src="start.png" style="width:100.0%" alt="start_point" />
<figcaption aria-hidden="true">start_point</figcaption>
</figure>

From this result we can show that like our expectation,
`maximum_speed_limit` is most importance among predictors. And speed is
almost increasing function to `maximum_speed_limit`. But unlike my
thinking, day of the week does not matter that much.

When we focus on end point, longitude is more important than latitude.
And end_latitude is not so much important when end_longitude is fixed.
When we view horizontally, I think the central traffic jam is the worst.
Finally interestingly, the starting point was not that different by
region compared to the ending point.

In this case, start and end point indicate different result. To show
more detail, I define start point + end point partial dependence
function.

``` r
pd_for<- function(start.latitude, start.longitude, end.latitude, end.longitude){
    library(tidyverse)
    library(xgboost)
    jeju_new <- jeju_use %>% mutate(
                                   start_latitude = rep(start.latitude, 2090071),
                                   start_longitude = rep(start.longitude, 2090071),
                                   end_latitude = rep(end.latitude, 2090071),
                                   end_longitude = rep(end.longitude, 2090071))
    jeju_total_new <- rbind(jeju_new, test)
    X <- model.matrix(target ~ . , data = jeju_total_new)[, -1]
    X_test <- X[2090072:2854903, ]
    X_use_new <- X[1:2090071, ]
    X_train_new <- X_use_new[-validation_index, ]
    
    xgb_new <- xgb.DMatrix(data = X_train_new, label = y_train)
    yhat_xgb_new <- predict(xgb_fit, xgb_new)
    mean(yhat_xgb_new)
}
```

This function calculate the point partial dependence of
`start.latitude, start.longitude, end.latitude, end.longitude`.

From this, I calculate inner to outer and outer to inner.

``` r
path_1 <- pd_for(33.45591, 126.5618, 33.38925, 126.2403) # inner -> outer
path_2 <- pd_for(33.38925, 126.2403, 33.45591, 126.5618) # outer -> inner
```

``` r
path_1
```

    ## [1] 68.98344

``` r
path_2
```

    ## [1] 38.19511

From this result we inference that to go inner place we need to more
time than inverse of that.

In figure 10, I illustrate the route of inner to outer and outer to
inner for several case. This figure supports my guess.

<figure>
<img src="arrow.png" style="width:100.0%"
alt="partial dependence of start and end point" />
<figcaption aria-hidden="true">partial dependence of start and end
point</figcaption>
</figure>

# Conclusion

We can get MSE for three model we fitted in this paper. First is OLS,
second Lasso regression finally XGBoost.

From this result show that we have many observation, nonlinear complex
model can outperform the linear model.

In this paper, we describe the elementary composition of XGBoost. From
this we know that it is important selection of method depend of data set
we have. These thinking can be applied to other machine learning method
like random forest, neural network and also in unsupervised learning
like clustering. From this paper, I want to say that there is no one
master key but we need to select the model by the situation you have.

# Reference

1.  Chen, Tianqi, and Carlos Guestrin. “XGBoost.” Proceedings of the
    22nd ACM SIGKDD International Conference on Knowledge Discovery and
    Data Mining (2016): 785-94.

2.  Hastie, Tibshirani, Friedman, Tibshirani, Robert, Friedman, J. H.,
    and SpringerLink. The Elements of Statistical Learning : Data
    Mining, Inference, and Prediction / Trevor Hastie, Robert
    Tibshirani, Jerome Friedman. (2009).

3.  Greenwell, Brandon,M. “Pdp: An R Package for Constructing Partial
    Dependence Plots.” The R Journal 9.1 (2017): 421.

4.  XGBoost Documentation

(<https://xgboost.readthedocs.io/en/latest/parameter.html>)
